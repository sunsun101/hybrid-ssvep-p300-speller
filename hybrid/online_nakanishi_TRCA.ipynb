{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 1,
=======
   "execution_count": 24,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "import  mne\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin, clone\n",
    "from numpy import ndarray\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.sparse import block_diag, identity, vstack, spmatrix\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy.linalg import eigh, pinv, qr\n",
    "from typing import Optional, List, cast\n",
    "from functools import partial\n",
    "from scipy.signal import filtfilt, cheb1ord, cheby1\n",
    "import scipy.linalg as linalg\n",
    "from pyriemann.utils.mean import mean_covariance\n",
    "from pyriemann.estimation import Covariances\n",
    "import pickle\n",
    "from brainflow import BoardShim"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 2,
=======
   "execution_count": 25,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_data(X):\n",
    "    \"\"\"Check data is numpy array and has the proper dimensions.\"\"\"\n",
    "    if not isinstance(X, (np.ndarray, list)):\n",
    "        raise AttributeError('data should be a list or a numpy array')\n",
    "\n",
    "    dtype = np.complex128 if np.any(np.iscomplex(X)) else np.float64\n",
    "    X = np.asanyarray(X, dtype=dtype)\n",
    "    if X.ndim > 3:\n",
    "        raise ValueError('Data must be 3D at most')\n",
    "\n",
    "    return X\n",
    "\n",
    "def theshapeof(X):\n",
    "    \"\"\"Return the shape of X.\"\"\"\n",
    "    X = _check_data(X)\n",
    "    # if not isinstance(X, np.ndarray):\n",
    "    #     raise AttributeError('X must be a numpy array')\n",
    "\n",
    "    if X.ndim == 3:\n",
    "        return X.shape[0], X.shape[1], X.shape[2]\n",
    "    elif X.ndim == 2:\n",
    "        return X.shape[0], X.shape[1], 1\n",
    "    elif X.ndim == 1:\n",
    "        return X.shape[0], 1, 1\n",
    "    else:\n",
    "        raise ValueError(\"Array contains more than 3 dimensions\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 3,
=======
   "execution_count": 26,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandpass(eeg, sfreq, Wp, Ws):\n",
    "    \"\"\"Filter bank design for decomposing EEG data into sub-band components.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    eeg : np.array, shape=(n_samples, n_chans[, n_trials])\n",
    "        Training data.\n",
    "    sfreq : int\n",
    "        Sampling frequency of the data.\n",
    "    Wp : 2-tuple\n",
    "        Passband for Chebyshev filter.\n",
    "    Ws : 2-tuple\n",
    "        Stopband for Chebyshev filter.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y: np.array, shape=(n_trials, n_chans, n_samples)\n",
    "        Sub-band components decomposed by a filter bank.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    scipy.signal.cheb1ord :\n",
    "        Chebyshev type I filter order selection.\n",
    "\n",
    "    \"\"\"\n",
    "    # Chebyshev type I filter order selection.\n",
    "    N, Wn = cheb1ord(Wp, Ws, 3, 40, fs=sfreq)\n",
    "\n",
    "    # Chebyshev type I filter design\n",
    "    B, A = cheby1(N, 0.5, Wn, btype=\"bandpass\", fs=sfreq)\n",
    "\n",
    "    # the arguments 'axis=0, padtype='odd', padlen=3*(max(len(B),len(A))-1)'\n",
    "    # correspond to Matlab filtfilt : https://dsp.stackexchange.com/a/47945\n",
    "    y = filtfilt(B, A, eeg, axis=0, padtype='odd',\n",
    "                 padlen=3 * (max(len(B), len(A)) - 1))\n",
    "    return y\n",
    "\n",
    "\n",
    "def schaefer_strimmer_cov(X):\n",
    "    r\"\"\"Schaefer-Strimmer covariance estimator.\n",
    "\n",
    "    Shrinkage estimator described in [1]_:\n",
    "\n",
    "    .. math:: \\hat{\\Sigma} = (1 - \\gamma)\\Sigma_{scm} + \\gamma T\n",
    "\n",
    "    where :math:`T` is the diagonal target matrix:\n",
    "\n",
    "    .. math:: T_{i,j} = \\{ \\Sigma_{scm}^{ii} \\text{if} i = j,\n",
    "         0 \\text{otherwise} \\}\n",
    "\n",
    "    Note that the optimal :math:`\\gamma` is estimated by the authors' method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: array, shape=(n_chans, n_samples)\n",
    "        Signal matrix.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cov: array, shape=(n_chans, n_chans)\n",
    "        Schaefer-Strimmer shrinkage covariance matrix.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Schafer, J., and K. Strimmer. 2005. A shrinkage approach to\n",
    "       large-scale covariance estimation and implications for functional\n",
    "       genomics. Statist. Appl. Genet. Mol. Biol. 4:32.\n",
    "    \"\"\"\n",
    "    ns = X.shape[1]\n",
    "    C_scm = np.cov(X, ddof=0)\n",
    "    X_c = X - np.tile(X.mean(axis=1), [ns, 1]).T\n",
    "\n",
    "    # Compute optimal gamma, the weigthing between SCM and srinkage estimator\n",
    "    R = ns / (ns - 1.0) * np.corrcoef(X)\n",
    "    var_R = (X_c ** 2).dot((X_c ** 2).T) - 2 * C_scm * X_c.dot(X_c.T)\n",
    "    var_R += ns * C_scm ** 2\n",
    "\n",
    "    var_R = ns / ((ns - 1) ** 3 * np.outer(X.var(1), X.var(1))) * var_R\n",
    "    R -= np.diag(np.diag(R))\n",
    "    var_R -= np.diag(np.diag(var_R))\n",
    "    gamma = max(0, min(1, var_R.sum() / (R ** 2).sum()))\n",
    "\n",
    "    cov = (1. - gamma) * (ns / (ns - 1.)) * C_scm\n",
    "    cov += gamma * (ns / (ns - 1.)) * np.diag(np.diag(C_scm))\n",
    "\n",
    "    return cov\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 4,
=======
   "execution_count": 27,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "def _trca(X):\n",
    "    \"\"\"Task-related component analysis.\n",
    "\n",
    "    This function implements the method described in [1]_.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape=(n_samples, n_chans[, n_trials])\n",
    "        Training data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    W : array, shape=(n_chans,)\n",
    "        Weight coefficients for electrodes which can be used as a spatial\n",
    "        filter.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] M. Nakanishi, Y. Wang, X. Chen, Y. -T. Wang, X. Gao, and T.-P. Jung,\n",
    "       \"Enhancing detection of SSVEPs for a high-speed brain speller using\n",
    "       task-related component analysis\", IEEE Trans. Biomed. Eng,\n",
    "       65(1):104-112, 2018.\n",
    "\n",
    "    \"\"\"\n",
    "    n_samples, n_chans, n_trials = theshapeof(X)\n",
    "\n",
    "    # 1. Compute empirical covariance of all data (to be bounded)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Concatenate all the trials to have all the data as a sequence\n",
    "    UX = np.zeros((n_chans, n_samples * n_trials))\n",
    "    for trial in range(n_trials):\n",
    "        UX[:, trial * n_samples:(trial + 1) * n_samples] = X[..., trial].T\n",
    "\n",
    "    # Mean centering\n",
    "    UX -= np.mean(UX, 1)[:, None]\n",
    "\n",
    "    # Covariance\n",
    "    Q = UX @ UX.T\n",
    "\n",
    "    # 2. Compute average empirical covariance between all pairs of trials\n",
    "    # -------------------------------------------------------------------------\n",
    "    S = np.zeros((n_chans, n_chans))\n",
    "    for trial_i in range(n_trials - 1):\n",
    "        x1 = np.squeeze(X[..., trial_i])\n",
    "\n",
    "        # Mean centering for the selected trial\n",
    "        x1 -= np.mean(x1, 0)\n",
    "\n",
    "        # Select a second trial that is different\n",
    "        for trial_j in range(trial_i + 1, n_trials):\n",
    "            x2 = np.squeeze(X[..., trial_j])\n",
    "\n",
    "            # Mean centering for the selected trial\n",
    "            x2 -= np.mean(x2, 0)\n",
    "\n",
    "            # Compute empirical covariance between the two selected trials and\n",
    "            # sum it\n",
    "            S = S + x1.T @ x2 + x2.T @ x1\n",
    "\n",
    "    # 3. Compute eigenvalues and vectors\n",
    "    # -------------------------------------------------------------------------\n",
    "    lambdas, W = linalg.eig(S, Q, left=True, right=False)\n",
    "\n",
    "    # Select the eigenvector corresponding to the biggest eigenvalue\n",
    "    W_best = W[:, np.argmax(lambdas)]\n",
    "\n",
    "    return W_best\n",
    "\n",
    "\n",
    "def trca_regul(X, method):\n",
    "    \"\"\"Task-related component analysis.\n",
    "\n",
    "    This function implements a variation of the method described in [1]_. It is\n",
    "    inspired by a riemannian geometry approach to CSP [2]_. It adds\n",
    "    regularization to the covariance matrices and uses the riemannian mean for\n",
    "    the inter-trial covariance matrix `S`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape=(n_samples, n_chans[, n_trials])\n",
    "        Training data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    W : array, shape=(n_chans,)\n",
    "        Weight coefficients for electrodes which can be used as a spatial\n",
    "        filter.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] M. Nakanishi, Y. Wang, X. Chen, Y. -T. Wang, X. Gao, and T.-P. Jung,\n",
    "       \"Enhancing detection of SSVEPs for a high-speed brain speller using\n",
    "       task-related component analysis\", IEEE Trans. Biomed. Eng,\n",
    "       65(1):104-112, 2018.\n",
    "    .. [2] Barachant, A., Bonnet, S., Congedo, M., & Jutten, C. (2010,\n",
    "       October). Common spatial pattern revisited by Riemannian geometry. In\n",
    "       2010 IEEE International Workshop on Multimedia Signal Processing (pp.\n",
    "       472-476). IEEE.\n",
    "\n",
    "    \"\"\"\n",
    "    n_samples, n_chans, n_trials = theshapeof(X)\n",
    "\n",
    "    # 1. Compute empirical covariance of all data (to be bounded)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Concatenate all the trials to have all the data as a sequence\n",
    "    UX = np.zeros((n_chans, n_samples * n_trials))\n",
    "    for trial in range(n_trials):\n",
    "        UX[:, trial * n_samples:(trial + 1) * n_samples] = X[..., trial].T\n",
    "\n",
    "    # Mean centering\n",
    "    UX -= np.mean(UX, 1)[:, None]\n",
    "\n",
    "    # Compute empirical variance of all data (to be bounded)\n",
    "    cov = Covariances(estimator=method).fit_transform(UX[np.newaxis, ...])\n",
    "    Q = np.squeeze(cov)\n",
    "\n",
    "    # 2. Compute average empirical covariance between all pairs of trials\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Intertrial correlation computation\n",
    "    data = np.concatenate((X, X), axis=1)\n",
    "\n",
    "    # Swapaxes to fit pyriemann Covariances\n",
    "    data = np.swapaxes(data, 0, 2)\n",
    "    cov = Covariances(estimator=method).fit_transform(data)\n",
    "\n",
    "    # Keep only inter-trial\n",
    "    S = cov[:, :n_chans, n_chans:] + cov[:, n_chans:, :n_chans]\n",
    "\n",
    "    # If the number of samples is too big, we compute an approximate of\n",
    "    # riemannian mean to speed up the computation\n",
    "    if n_trials < 30:\n",
    "        S = mean_covariance(S, metric='riemann')\n",
    "    else:\n",
    "        S = mean_covariance(S, metric='logeuclid')\n",
    "\n",
    "    # 3. Compute eigenvalues and vectors\n",
    "    # -------------------------------------------------------------------------\n",
    "    lambdas, W = linalg.eig(S, Q, left=True, right=False)\n",
    "\n",
    "    # Select the eigenvector corresponding to the biggest eigenvalue\n",
    "    W_best = W[:, np.argmax(lambdas)]\n",
    "\n",
    "    return W_best\n",
    "\n",
    "\n",
    "\n",
    "def get_corr(a,b, latency=20):\n",
    "    cross_correlation = abs(np.correlate(a,b, mode='same'))\n",
    "    center_idx = len(cross_correlation) // 2\n",
    "    max_corr = cross_correlation[center_idx-latency  : center_idx+latency].max()\n",
    "    return max_corr\n",
    "\n",
    "def trca_crosscorrelation(X):\n",
    "    latency=250\n",
    "    n_samples, n_chans, n_trials = theshapeof(X)\n",
    "\n",
    "    # 1. Compute empirical covariance of all data (to be bounded)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Concatenate all the trials to have all the data as a sequence\n",
    "    UX = np.zeros((n_chans, n_samples * n_trials))\n",
    "    for trial in range(n_trials):\n",
    "        UX[:, trial * n_samples:(trial + 1) * n_samples] = X[..., trial].T\n",
    "\n",
    "    # Mean centering\n",
    "    UX -= np.mean(UX, 1)[:, None]\n",
    "\n",
    "    # Covariance\n",
    "    # Q = UX @ UX.T\n",
    "    # Use my cross correlation\n",
    "    Q = np.zeros((UX.shape[0],UX.shape[0]))\n",
    "    for i in range(UX.shape[0]):\n",
    "        for j in range(UX.shape[0]):\n",
    "            a = UX[i]\n",
    "            b = UX[j]\n",
    "            Q[i,j] = get_corr(a,b,latency=latency)\n",
    "    # 2. Compute average empirical covariance between all pairs of trials\n",
    "    # -------------------------------------------------------------------------\n",
    "    S = np.zeros((n_chans, n_chans))\n",
    "    for i in range(n_chans):\n",
    "        for j in range(n_chans):\n",
    "            # n_samples, n_chans, n_trials \n",
    "            x_i = X[:, i, :]\n",
    "            x_j = X[:, j, :]\n",
    "            # print(f\"{x_i.shape=}\")\n",
    "            # print(f\"{x_j.shape=}\")\n",
    "            for t1 in range(n_trials):\n",
    "                for t2 in range(n_trials):\n",
    "                    if(t1 == t2): continue\n",
    "                    x_i_t1 = np.squeeze(x_i[:,t1])\n",
    "                    x_i_t1 -= x_i_t1.mean()\n",
    "\n",
    "                    x_j_t2 = np.squeeze(x_j[:,t2])\n",
    "                    x_j_t2 -= x_j_t2.mean()\n",
    "                    \n",
    "                    S[i,j] += get_corr(x_i_t1, x_j_t2, latency=latency)\n",
    "    # for trial_i in range(n_trials - 1):\n",
    "    #     x1 = np.squeeze(X[..., trial_i])\n",
    "\n",
    "    #     # Mean centering for the selected trial\n",
    "    #     x1 -= np.mean(x1, 0)\n",
    "\n",
    "    #     # Select a second trial that is different\n",
    "    #     for trial_j in range(trial_i + 1, n_trials):\n",
    "    #         x2 = np.squeeze(X[..., trial_j])\n",
    "\n",
    "    #         # Mean centering for the selected trial\n",
    "    #         x2 -= np.mean(x2, 0)\n",
    "\n",
    "    #         # Compute empirical covariance between the two selected trials and\n",
    "    #         # sum it\n",
    "    #         S = S + x1.T @ x2 + x2.T @ x1\n",
    "\n",
    "    # 3. Compute eigenvalues and vectors\n",
    "    # -------------------------------------------------------------------------\n",
    "    lambdas, W = linalg.eig(S, Q, left=True, right=False)\n",
    "\n",
    "    # Select the eigenvector corresponding to the biggest eigenvalue\n",
    "    W_best = W[:, np.argmax(lambdas)]\n",
    "\n",
    "    return W_best"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 5,
=======
   "execution_count": 28,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRCA:\n",
    "    \"\"\"Task-Related Component Analysis (TRCA).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sfreq : float\n",
    "        Sampling rate.\n",
    "    filterbank : list[[2-tuple, 2-tuple]]\n",
    "        Filterbank frequencies. Each list element is itself a list of passband\n",
    "        `Wp` and stopband `Ws` edges frequencies `[Wp, Ws]`. For example, this\n",
    "        creates 3 bands, starting at 6, 14, and 22 hz respectively::\n",
    "\n",
    "            [[(6, 90), (4, 100)],\n",
    "             [(14, 90), (10, 100)],\n",
    "             [(22, 90), (16, 100)]]\n",
    "\n",
    "        See :func:`scipy.signal.cheb1ord()` for more information on how to\n",
    "        specify the `Wp` and `Ws`.\n",
    "    ensemble : bool\n",
    "        If True, perform the ensemble TRCA analysis (default=False).\n",
    "    method : str in {'original'| 'riemann'}\n",
    "        Use original implementation from [1]_ or a variation that uses\n",
    "        regularization and the geodesic mean [2]_.\n",
    "    regularization : str in {'schaefer' | 'lwf' | 'oas' | 'scm'}\n",
    "        Regularization estimator used for covariance estimation with the\n",
    "        `riemann` method. Consider 'schaefer', 'lwf', 'oas'. 'scm' does not add\n",
    "        regularization and is almost equivalent to the original implementation.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    traindata : array, shape=(n_bands, n_chans, n_trials)\n",
    "        Reference (training) data decomposed into sub-band components by the\n",
    "        filter bank analysis.\n",
    "    y_train : array, shape=(n_trials)\n",
    "        Labels associated with the train data.\n",
    "    coef_ : array, shape=(n_chans, n_chans)\n",
    "        Weight coefficients for electrodes which can be used as a spatial\n",
    "        filter.\n",
    "    classes : list\n",
    "        Classes.\n",
    "    n_bands : int\n",
    "        Number of sub-bands.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] M. Nakanishi, Y. Wang, X. Chen, Y. -T. Wang, X. Gao, and T.-P. Jung,\n",
    "       \"Enhancing detection of SSVEPs for a high-speed brain speller using\n",
    "       task-related component analysis\", IEEE Trans. Biomed. Eng,\n",
    "       65(1):104-112, 2018.\n",
    "    .. [2] Barachant, A., Bonnet, S., Congedo, M., & Jutten, C. (2010,\n",
    "       October). Common spatial pattern revisited by Riemannian geometry. In\n",
    "       2010 IEEE International Workshop on Multimedia Signal Processing (pp.\n",
    "       472-476). IEEE.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sfreq, filterbank, ensemble=False, method='original',\n",
    "                 estimator='scm'):\n",
    "        self.sfreq = sfreq\n",
    "        self.ensemble = ensemble\n",
    "        self.filterbank = filterbank\n",
    "        self.n_bands = len(self.filterbank)\n",
    "        self.coef_ = None\n",
    "        self.method = method\n",
    "        if estimator == 'schaefer':\n",
    "            self.estimator = schaefer_strimmer_cov\n",
    "        else:\n",
    "            self.estimator = estimator\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Training stage of the TRCA-based SSVEP detection.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array, shape=(n_samples, n_chans[, n_trials])\n",
    "            Training EEG data.\n",
    "        y : array, shape=(trials,)\n",
    "            True label corresponding to each trial of the data array.\n",
    "\n",
    "        \"\"\"\n",
    "        n_samples, n_chans, _ = theshapeof(X)\n",
    "        classes = np.unique(y)\n",
    "\n",
    "        trains = np.zeros((len(classes), self.n_bands, n_samples, n_chans))\n",
    "\n",
    "        W = np.zeros((self.n_bands, len(classes), n_chans))\n",
    "\n",
    "        for class_i in classes:\n",
    "            # Select data with a specific label\n",
    "            eeg_tmp = X[..., y == class_i]\n",
    "            for fb_i in range(self.n_bands):\n",
    "                # Filter the signal with fb_i\n",
    "                eeg_tmp = bandpass(eeg_tmp, self.sfreq,\n",
    "                                   Wp=self.filterbank[fb_i][0],\n",
    "                                   Ws=self.filterbank[fb_i][1])\n",
    "                if (eeg_tmp.ndim == 3):\n",
    "                    # Compute mean of the signal across trials\n",
    "                    trains[class_i, fb_i] = np.mean(eeg_tmp, -1)\n",
    "                else:\n",
    "                    trains[class_i, fb_i] = eeg_tmp\n",
    "                # Find the spatial filter for the corresponding filtered signal\n",
    "                # and label\n",
    "                if self.method == 'original':\n",
    "                    w_best = _trca(eeg_tmp)\n",
    "                elif self.method == 'riemann':\n",
    "                    w_best = trca_regul(eeg_tmp, self.estimator)\n",
    "                elif self.method == 'crosscorrelation':\n",
    "                    w_best = trca_crosscorrelation(eeg_tmp)\n",
    "                else:\n",
    "                    raise ValueError('Invalid `method` option.')\n",
    "\n",
    "                W[fb_i, class_i, :] = w_best  # Store the spatial filter\n",
    "\n",
    "        self.trains = trains\n",
    "        self.coef_ = W\n",
    "        self.classes = classes\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Test phase of the TRCA-based SSVEP detection.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array, shape=(n_samples, n_chans[, n_trials])\n",
    "            Test data.\n",
    "        model: dict\n",
    "            Fitted model to be used in testing phase.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pred: np.array, shape (trials)\n",
    "            The target estimated by the method.\n",
    "\n",
    "        \"\"\"\n",
    "        if self.coef_ is None:\n",
    "            raise RuntimeError('TRCA is not fitted')\n",
    "\n",
    "        # Alpha coefficients for the fusion of filterbank analysis\n",
    "        fb_coefs = [(x + 1)**(-1.25) + 0.25 for x in range(self.n_bands)]\n",
    "        _, _, n_trials = theshapeof(X)\n",
    "\n",
    "        r = np.zeros((self.n_bands, len(self.classes)))\n",
    "        pred = np.zeros((n_trials), 'int')  # To store predictions\n",
    "\n",
    "        for trial in range(n_trials):\n",
    "            test_tmp = X[..., trial]  # pick a trial to be analysed\n",
    "            for fb_i in range(self.n_bands):\n",
    "\n",
    "                # filterbank on testdata\n",
    "                testdata = bandpass(test_tmp, self.sfreq,\n",
    "                                    Wp=self.filterbank[fb_i][0],\n",
    "                                    Ws=self.filterbank[fb_i][1])\n",
    "\n",
    "                for class_i in self.classes:\n",
    "                    # Retrieve reference signal for class i\n",
    "                    # (shape: n_chans, n_samples)\n",
    "                    traindata = np.squeeze(self.trains[class_i, fb_i])\n",
    "                    if self.ensemble:\n",
    "                        # shape = (n_chans, n_classes)\n",
    "                        w = np.squeeze(self.coef_[fb_i]).T\n",
    "                    else:\n",
    "                        # shape = (n_chans)\n",
    "                        w = np.squeeze(self.coef_[fb_i, class_i])\n",
    "\n",
    "                    # Compute 2D correlation of spatially filtered test data\n",
    "                    # with ref\n",
    "                    r_tmp = np.corrcoef((testdata @ w).flatten(),\n",
    "                                        (traindata @ w).flatten())\n",
    "                    r[fb_i, class_i] = r_tmp[0, 1]\n",
    "\n",
    "            rho = np.dot(fb_coefs, r)  # fusion for the filterbank analysis\n",
    "\n",
    "            tau = np.argmax(rho)  # retrieving index of the max\n",
    "            pred[trial] = int(tau)\n",
    "\n",
    "        return pred\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 13,
=======
   "execution_count": 29,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
      "(9, 2028)\n",
      "(9, 2021)\n",
      "(9, 2030)\n",
      "(9, 2006)\n",
      "(9, 2014)\n",
      "(9, 2006)\n",
      "(9, 2021)\n",
      "(9, 2032)\n",
      "(9, 2014)\n",
      "(9, 2008)\n",
      "(9, 2019)\n",
      "(9, 2012)\n",
      "(9, 2034)\n",
      "(9, 2019)\n",
      "(9, 2032)\n",
      "(9, 2014)\n",
      "(16, 8, 1240) (16,)\n"
=======
      "(9, 2220)\n",
      "(9, 2278)\n",
      "(9, 2007)\n",
      "(9, 2366)\n",
      "(9, 2196)\n",
      "(9, 2223)\n",
      "(9, 2232)\n",
      "(9, 2242)\n",
      "(9, 2215)\n",
      "(9, 2083)\n",
      "(9, 2045)\n",
      "(9, 2021)\n",
      "(9, 2382)\n",
      "(9, 2011)\n",
      "(9, 2313)\n",
      "(9, 2687)\n",
      "(9, 2038)\n",
      "(9, 2047)\n",
      "(9, 2131)\n",
      "(9, 2007)\n",
      "(9, 2039)\n",
      "(9, 2279)\n",
      "(9, 2027)\n",
      "(9, 2007)\n",
      "(9, 2413)\n",
      "(9, 2009)\n",
      "(9, 2077)\n",
      "(9, 2254)\n",
      "(9, 2373)\n",
      "(9, 2032)\n",
      "(9, 2135)\n",
      "(9, 2018)\n",
      "(32, 8, 1700) (32,)\n"
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "from scipy import signal\n",
    "\n",
    "BOARD_ID = 8\n",
    "loaded_model = pickle.load(open(r\"E:\\Thesis\\HybridSpeller\\hybrid\\nakanishi_TRCA_model.sav\", 'rb'))\n",
    "# pickle_files = glob(r'E:\\Thesis\\HybridSpeller\\nine_flicker\\record\\farheen_20230427_v2\\*.pickle')\n",
<<<<<<< Updated upstream
    "pickle_files = glob(r'C:\\Users\\bci\\Documents\\projects\\hybrid-ssvep-p300-speller\\hybrid\\record\\trials\\sunsun_20230605_2sec_0.5_overlap_12Hz_v3\\*.pickle')\n",
=======
    "pickle_files = glob(r'E:\\Thesis\\HybridSpeller\\hybrid\\record\\final\\pramod_20230704_2sec_0.5_overlap_8Hz_16target\\*.pickle')\n",
>>>>>>> Stashed changes
    "\n",
    "y_pickle = []\n",
    "X_pickle = []\n",
    "for fpath in pickle_files:\n",
    "    _,filename = os.path.split(fpath)\n",
    "    y_pickle.append(filename[1])\n",
    "\n",
    "    with open(fpath, 'rb') as handle:\n",
    "        pickle_data = pickle.load(handle)\n",
    "\n",
    "    marker_channel = BoardShim.get_marker_channel(BOARD_ID)\n",
    "    eeg_channels = BoardShim.get_eeg_channels(BOARD_ID)\n",
    "    pickle_data[eeg_channels] = pickle_data[eeg_channels] / 1e6\n",
    "    pickle_data = pickle_data[eeg_channels + [marker_channel]]\n",
    "    print(pickle_data.shape)\n",
    "\n",
    "    _CHANNELS = ['FZ', 'C3', 'CZ', 'C4', 'PZ', 'PO7', 'OZ', 'PO8']\n",
    "\n",
    "    data = pickle_data[:8,:1700]\n",
    "\n",
    "\n",
    "    b,a = signal.iirfilter(7, Wn=[1, 92], rp=0.5, btype='band', analog=False, fs=250,  ftype='cheby1')\n",
    "    data = signal.filtfilt(b,a,data,axis=1)\n",
    "\n",
    "    notch_freq = 50\n",
    "    quality = 1\n",
    "    b,a = signal.iirnotch(notch_freq, quality, fs=250)\n",
    "    for i in range(8):\n",
    "        data[i] = signal.lfilter(b, a, data[i])\n",
    "\n",
    "    X_pickle.append(np.expand_dims(data[:],axis=0))\n",
    "\n",
    "y_pickle = np.array(y_pickle)\n",
    "X_pickle = np.concatenate(X_pickle)\n",
    "\n",
    "print(X_pickle.shape, y_pickle.shape)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 8, 1700)\n"
     ]
    }
   ],
>>>>>>> Stashed changes
   "source": [
    "print(X_pickle.shape)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'A' 'B'\n",
      " 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P']\n"
     ]
    }
   ],
>>>>>>> Stashed changes
   "source": [
    "print(y_pickle)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 32,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_predict = None\n",
    "# best_score = 0\n",
    "# X_pickle = np.swapaxes(X_pickle,0,2)\n",
    "# print(X_pickle.shape)\n",
    "# for r in range(500):\n",
    "#     offset = int(r)\n",
    "#     # offset = int(r)\n",
    "#     preds_pickle = loaded_model.predict(X_pickle[offset:offset + 1250,:,:])\n",
    "#     # print(preds_pickle)\n",
    "#     # print(y_pickle)\n",
    "#     # print(X_pickle[:,:,offset:offset + 750].shape)\n",
    "#     c = 0\n",
    "#     preds_pickle_converted = []\n",
    "#     for idx in range(len(preds_pickle)):\n",
    "#         pred = preds_pickle[idx]\n",
    "#         label = y_pickle[idx]\n",
    "#         if(pred == 0):\n",
    "#             pred = 'A'\n",
    "#         if(pred == 1):\n",
    "#             pred = 'B'\n",
    "#         if(pred == 2):\n",
    "#             pred = 'C'\n",
    "#         if(pred == 3):\n",
    "#             pred = 'D'\n",
    "#         if(pred == 4):\n",
    "#             pred = 'E'\n",
    "#         if(pred == 5):\n",
    "#             pred = 'F'\n",
    "#         if(pred == 6):\n",
    "#             pred = 'G'\n",
    "#         if(pred == 7):\n",
    "#             pred = 'H'\n",
    "#         if(pred == 8):\n",
    "#             pred = 'I'\n",
    "#         if(pred == 9):\n",
    "#             pred = 'J'\n",
    "#         if(pred == 10):\n",
    "#             pred = 'K'\n",
    "#         if(pred == 11):\n",
    "#             pred = 'L'\n",
    "#         if(pred == 12):\n",
    "#             pred = 'M'\n",
    "#         if(pred == 13):\n",
    "#             pred = 'N'\n",
    "#         if(pred == 14):\n",
    "#             pred = 'O'\n",
    "#         if(pred == 15):\n",
    "#             pred = 'P'\n",
    "#         if(label == pred):\n",
    "#             c += 1\n",
    "#         # print(idx, f\"{pred=} {label=}\" )\n",
    "#     score = c/len(preds_pickle)\n",
    "#     print(offset, score)\n",
    "#     if(best_score < score):\n",
    "#         best_score = score\n",
    "#         best_predict = preds_pickle_converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 8, 1700)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pickle.shape"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1700, 8, 32)\n",
      "[12  8  2 12  4  0  0 15 11  9 12 11 11  9  6 11  0  3  5  3  0  3  3  7\n",
      "  0  9 10 11  6 15 14 12]\n",
      "0 pred='M' 12 label='A' 0\n",
      "1 pred='I' 8 label='B' 1\n",
      "2 pred='C' 2 label='C' 2\n",
      "3 pred='M' 12 label='D' 3\n",
      "4 pred='E' 4 label='E' 4\n",
      "5 pred='A' 0 label='F' 5\n",
      "6 pred='A' 0 label='G' 6\n",
      "7 pred='P' 15 label='H' 7\n",
      "8 pred='L' 11 label='I' 8\n",
      "9 pred='J' 9 label='J' 9\n",
      "10 pred='M' 12 label='K' 10\n",
      "11 pred='L' 11 label='L' 11\n",
      "12 pred='L' 11 label='M' 12\n",
      "13 pred='J' 9 label='N' 13\n",
      "14 pred='G' 6 label='O' 14\n",
      "15 pred='L' 11 label='P' 15\n",
      "16 pred='A' 0 label='A' 0\n",
      "17 pred='D' 3 label='B' 1\n",
      "18 pred='F' 5 label='C' 2\n",
      "19 pred='D' 3 label='D' 3\n",
      "20 pred='A' 0 label='E' 4\n",
      "21 pred='D' 3 label='F' 5\n",
      "22 pred='D' 3 label='G' 6\n",
      "23 pred='H' 7 label='H' 7\n",
      "24 pred='A' 0 label='I' 8\n",
      "25 pred='J' 9 label='J' 9\n",
      "26 pred='K' 10 label='K' 10\n",
      "27 pred='L' 11 label='L' 11\n",
      "28 pred='G' 6 label='M' 12\n",
      "29 pred='P' 15 label='N' 13\n",
      "30 pred='O' 14 label='O' 14\n",
      "31 pred='M' 12 label='P' 15\n",
      "0.34375\n",
      "32\n"
     ]
    }
   ],
>>>>>>> Stashed changes
   "source": [
    "offset = 225\n",
    "X_pickle = np.swapaxes(X_pickle,0,2)\n",
    "print(X_pickle.shape)\n",
    "preds_pickle = loaded_model.predict(X_pickle[offset:offset + 1250,:,:])\n",
    "print(preds_pickle)\n",
    "# print(y_pickle)\n",
    "c = 0\n",
    "for idx in range(len(preds_pickle)):\n",
    "    pred = preds_pickle[idx]\n",
    "    label = y_pickle[idx]\n",
    "    if(pred == 0):\n",
    "        pred = 'A'\n",
    "    if(pred == 1):\n",
    "        pred = 'B'\n",
    "    if(pred == 2):\n",
    "        pred = 'C'\n",
    "    if(pred == 3):\n",
    "        pred = 'D'\n",
    "    if(pred == 4):\n",
    "        pred = 'E'\n",
    "    if(pred == 5):\n",
    "        pred = 'F'\n",
    "    if(pred == 6):\n",
    "        pred = 'G'\n",
    "    if(pred == 7):\n",
    "        pred = 'H'\n",
    "    if(pred == 8):\n",
    "        pred = 'I'\n",
    "    if(pred == 9):\n",
    "        pred = 'J'\n",
    "    if(pred == 10):\n",
    "        pred = 'K'\n",
    "    if(pred == 11):\n",
    "        pred = 'L'\n",
    "    if(pred == 12):\n",
    "        pred = 'M'\n",
    "    if(pred == 13):\n",
    "        pred = 'N'\n",
    "    if(pred == 14):\n",
    "        pred = 'O'\n",
    "    if(pred == 15):\n",
    "        pred = 'P'\n",
    "    if(label == pred):\n",
    "        c += 1\n",
    "    pred_ascii = ord(pred)-65\n",
    "    label_ascii = ord(label)-65\n",
    "    print(idx, f\"{pred=} {pred_ascii} {label=} {label_ascii}\" )\n",
    "print(c/len(preds_pickle)\n",
    "      )\n",
    "print(len(preds_pickle))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HybridSpeller-q8UBACmb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
