{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  mne\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin, clone\n",
    "from numpy import ndarray\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.sparse import block_diag, identity, vstack, spmatrix\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy.linalg import eigh, pinv, qr\n",
    "from typing import Optional, List, cast\n",
    "from functools import partial\n",
    "from scipy.signal import filtfilt, cheb1ord, cheby1\n",
    "import scipy.linalg as linalg\n",
    "from pyriemann.utils.mean import mean_covariance\n",
    "from pyriemann.estimation import Covariances\n",
    "import pickle\n",
    "from brainflow import BoardShim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_data(X):\n",
    "    \"\"\"Check data is numpy array and has the proper dimensions.\"\"\"\n",
    "    if not isinstance(X, (np.ndarray, list)):\n",
    "        raise AttributeError('data should be a list or a numpy array')\n",
    "\n",
    "    dtype = np.complex128 if np.any(np.iscomplex(X)) else np.float64\n",
    "    X = np.asanyarray(X, dtype=dtype)\n",
    "    if X.ndim > 3:\n",
    "        raise ValueError('Data must be 3D at most')\n",
    "\n",
    "    return X\n",
    "\n",
    "def theshapeof(X):\n",
    "    \"\"\"Return the shape of X.\"\"\"\n",
    "    X = _check_data(X)\n",
    "    # if not isinstance(X, np.ndarray):\n",
    "    #     raise AttributeError('X must be a numpy array')\n",
    "\n",
    "    if X.ndim == 3:\n",
    "        return X.shape[0], X.shape[1], X.shape[2]\n",
    "    elif X.ndim == 2:\n",
    "        return X.shape[0], X.shape[1], 1\n",
    "    elif X.ndim == 1:\n",
    "        return X.shape[0], 1, 1\n",
    "    else:\n",
    "        raise ValueError(\"Array contains more than 3 dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandpass(eeg, sfreq, Wp, Ws):\n",
    "    \"\"\"Filter bank design for decomposing EEG data into sub-band components.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    eeg : np.array, shape=(n_samples, n_chans[, n_trials])\n",
    "        Training data.\n",
    "    sfreq : int\n",
    "        Sampling frequency of the data.\n",
    "    Wp : 2-tuple\n",
    "        Passband for Chebyshev filter.\n",
    "    Ws : 2-tuple\n",
    "        Stopband for Chebyshev filter.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y: np.array, shape=(n_trials, n_chans, n_samples)\n",
    "        Sub-band components decomposed by a filter bank.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    scipy.signal.cheb1ord :\n",
    "        Chebyshev type I filter order selection.\n",
    "\n",
    "    \"\"\"\n",
    "    # Chebyshev type I filter order selection.\n",
    "    N, Wn = cheb1ord(Wp, Ws, 3, 40, fs=sfreq)\n",
    "\n",
    "    # Chebyshev type I filter design\n",
    "    B, A = cheby1(N, 0.5, Wn, btype=\"bandpass\", fs=sfreq)\n",
    "\n",
    "    # the arguments 'axis=0, padtype='odd', padlen=3*(max(len(B),len(A))-1)'\n",
    "    # correspond to Matlab filtfilt : https://dsp.stackexchange.com/a/47945\n",
    "    y = filtfilt(B, A, eeg, axis=0, padtype='odd',\n",
    "                 padlen=3 * (max(len(B), len(A)) - 1))\n",
    "    return y\n",
    "\n",
    "\n",
    "def schaefer_strimmer_cov(X):\n",
    "    r\"\"\"Schaefer-Strimmer covariance estimator.\n",
    "\n",
    "    Shrinkage estimator described in [1]_:\n",
    "\n",
    "    .. math:: \\hat{\\Sigma} = (1 - \\gamma)\\Sigma_{scm} + \\gamma T\n",
    "\n",
    "    where :math:`T` is the diagonal target matrix:\n",
    "\n",
    "    .. math:: T_{i,j} = \\{ \\Sigma_{scm}^{ii} \\text{if} i = j,\n",
    "         0 \\text{otherwise} \\}\n",
    "\n",
    "    Note that the optimal :math:`\\gamma` is estimated by the authors' method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: array, shape=(n_chans, n_samples)\n",
    "        Signal matrix.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cov: array, shape=(n_chans, n_chans)\n",
    "        Schaefer-Strimmer shrinkage covariance matrix.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Schafer, J., and K. Strimmer. 2005. A shrinkage approach to\n",
    "       large-scale covariance estimation and implications for functional\n",
    "       genomics. Statist. Appl. Genet. Mol. Biol. 4:32.\n",
    "    \"\"\"\n",
    "    ns = X.shape[1]\n",
    "    C_scm = np.cov(X, ddof=0)\n",
    "    X_c = X - np.tile(X.mean(axis=1), [ns, 1]).T\n",
    "\n",
    "    # Compute optimal gamma, the weigthing between SCM and srinkage estimator\n",
    "    R = ns / (ns - 1.0) * np.corrcoef(X)\n",
    "    var_R = (X_c ** 2).dot((X_c ** 2).T) - 2 * C_scm * X_c.dot(X_c.T)\n",
    "    var_R += ns * C_scm ** 2\n",
    "\n",
    "    var_R = ns / ((ns - 1) ** 3 * np.outer(X.var(1), X.var(1))) * var_R\n",
    "    R -= np.diag(np.diag(R))\n",
    "    var_R -= np.diag(np.diag(var_R))\n",
    "    gamma = max(0, min(1, var_R.sum() / (R ** 2).sum()))\n",
    "\n",
    "    cov = (1. - gamma) * (ns / (ns - 1.)) * C_scm\n",
    "    cov += gamma * (ns / (ns - 1.)) * np.diag(np.diag(C_scm))\n",
    "\n",
    "    return cov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _trca(X):\n",
    "    \"\"\"Task-related component analysis.\n",
    "\n",
    "    This function implements the method described in [1]_.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape=(n_samples, n_chans[, n_trials])\n",
    "        Training data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    W : array, shape=(n_chans,)\n",
    "        Weight coefficients for electrodes which can be used as a spatial\n",
    "        filter.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] M. Nakanishi, Y. Wang, X. Chen, Y. -T. Wang, X. Gao, and T.-P. Jung,\n",
    "       \"Enhancing detection of SSVEPs for a high-speed brain speller using\n",
    "       task-related component analysis\", IEEE Trans. Biomed. Eng,\n",
    "       65(1):104-112, 2018.\n",
    "\n",
    "    \"\"\"\n",
    "    n_samples, n_chans, n_trials = theshapeof(X)\n",
    "\n",
    "    # 1. Compute empirical covariance of all data (to be bounded)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Concatenate all the trials to have all the data as a sequence\n",
    "    UX = np.zeros((n_chans, n_samples * n_trials))\n",
    "    for trial in range(n_trials):\n",
    "        UX[:, trial * n_samples:(trial + 1) * n_samples] = X[..., trial].T\n",
    "\n",
    "    # Mean centering\n",
    "    UX -= np.mean(UX, 1)[:, None]\n",
    "\n",
    "    # Covariance\n",
    "    Q = UX @ UX.T\n",
    "\n",
    "    # 2. Compute average empirical covariance between all pairs of trials\n",
    "    # -------------------------------------------------------------------------\n",
    "    S = np.zeros((n_chans, n_chans))\n",
    "    for trial_i in range(n_trials - 1):\n",
    "        x1 = np.squeeze(X[..., trial_i])\n",
    "\n",
    "        # Mean centering for the selected trial\n",
    "        x1 -= np.mean(x1, 0)\n",
    "\n",
    "        # Select a second trial that is different\n",
    "        for trial_j in range(trial_i + 1, n_trials):\n",
    "            x2 = np.squeeze(X[..., trial_j])\n",
    "\n",
    "            # Mean centering for the selected trial\n",
    "            x2 -= np.mean(x2, 0)\n",
    "\n",
    "            # Compute empirical covariance between the two selected trials and\n",
    "            # sum it\n",
    "            S = S + x1.T @ x2 + x2.T @ x1\n",
    "\n",
    "    # 3. Compute eigenvalues and vectors\n",
    "    # -------------------------------------------------------------------------\n",
    "    lambdas, W = linalg.eig(S, Q, left=True, right=False)\n",
    "\n",
    "    # Select the eigenvector corresponding to the biggest eigenvalue\n",
    "    W_best = W[:, np.argmax(lambdas)]\n",
    "\n",
    "    return W_best\n",
    "\n",
    "\n",
    "def trca_regul(X, method):\n",
    "    \"\"\"Task-related component analysis.\n",
    "\n",
    "    This function implements a variation of the method described in [1]_. It is\n",
    "    inspired by a riemannian geometry approach to CSP [2]_. It adds\n",
    "    regularization to the covariance matrices and uses the riemannian mean for\n",
    "    the inter-trial covariance matrix `S`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape=(n_samples, n_chans[, n_trials])\n",
    "        Training data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    W : array, shape=(n_chans,)\n",
    "        Weight coefficients for electrodes which can be used as a spatial\n",
    "        filter.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] M. Nakanishi, Y. Wang, X. Chen, Y. -T. Wang, X. Gao, and T.-P. Jung,\n",
    "       \"Enhancing detection of SSVEPs for a high-speed brain speller using\n",
    "       task-related component analysis\", IEEE Trans. Biomed. Eng,\n",
    "       65(1):104-112, 2018.\n",
    "    .. [2] Barachant, A., Bonnet, S., Congedo, M., & Jutten, C. (2010,\n",
    "       October). Common spatial pattern revisited by Riemannian geometry. In\n",
    "       2010 IEEE International Workshop on Multimedia Signal Processing (pp.\n",
    "       472-476). IEEE.\n",
    "\n",
    "    \"\"\"\n",
    "    n_samples, n_chans, n_trials = theshapeof(X)\n",
    "\n",
    "    # 1. Compute empirical covariance of all data (to be bounded)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Concatenate all the trials to have all the data as a sequence\n",
    "    UX = np.zeros((n_chans, n_samples * n_trials))\n",
    "    for trial in range(n_trials):\n",
    "        UX[:, trial * n_samples:(trial + 1) * n_samples] = X[..., trial].T\n",
    "\n",
    "    # Mean centering\n",
    "    UX -= np.mean(UX, 1)[:, None]\n",
    "\n",
    "    # Compute empirical variance of all data (to be bounded)\n",
    "    cov = Covariances(estimator=method).fit_transform(UX[np.newaxis, ...])\n",
    "    Q = np.squeeze(cov)\n",
    "\n",
    "    # 2. Compute average empirical covariance between all pairs of trials\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Intertrial correlation computation\n",
    "    data = np.concatenate((X, X), axis=1)\n",
    "\n",
    "    # Swapaxes to fit pyriemann Covariances\n",
    "    data = np.swapaxes(data, 0, 2)\n",
    "    cov = Covariances(estimator=method).fit_transform(data)\n",
    "\n",
    "    # Keep only inter-trial\n",
    "    S = cov[:, :n_chans, n_chans:] + cov[:, n_chans:, :n_chans]\n",
    "\n",
    "    # If the number of samples is too big, we compute an approximate of\n",
    "    # riemannian mean to speed up the computation\n",
    "    if n_trials < 30:\n",
    "        S = mean_covariance(S, metric='riemann')\n",
    "    else:\n",
    "        S = mean_covariance(S, metric='logeuclid')\n",
    "\n",
    "    # 3. Compute eigenvalues and vectors\n",
    "    # -------------------------------------------------------------------------\n",
    "    lambdas, W = linalg.eig(S, Q, left=True, right=False)\n",
    "\n",
    "    # Select the eigenvector corresponding to the biggest eigenvalue\n",
    "    W_best = W[:, np.argmax(lambdas)]\n",
    "\n",
    "    return W_best\n",
    "\n",
    "\n",
    "\n",
    "def get_corr(a,b, latency=20):\n",
    "    cross_correlation = abs(np.correlate(a,b, mode='same'))\n",
    "    center_idx = len(cross_correlation) // 2\n",
    "    max_corr = cross_correlation[center_idx-latency  : center_idx+latency].max()\n",
    "    return max_corr\n",
    "\n",
    "def trca_crosscorrelation(X):\n",
    "    latency=250\n",
    "    n_samples, n_chans, n_trials = theshapeof(X)\n",
    "\n",
    "    # 1. Compute empirical covariance of all data (to be bounded)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Concatenate all the trials to have all the data as a sequence\n",
    "    UX = np.zeros((n_chans, n_samples * n_trials))\n",
    "    for trial in range(n_trials):\n",
    "        UX[:, trial * n_samples:(trial + 1) * n_samples] = X[..., trial].T\n",
    "\n",
    "    # Mean centering\n",
    "    UX -= np.mean(UX, 1)[:, None]\n",
    "\n",
    "    # Covariance\n",
    "    # Q = UX @ UX.T\n",
    "    # Use my cross correlation\n",
    "    Q = np.zeros((UX.shape[0],UX.shape[0]))\n",
    "    for i in range(UX.shape[0]):\n",
    "        for j in range(UX.shape[0]):\n",
    "            a = UX[i]\n",
    "            b = UX[j]\n",
    "            Q[i,j] = get_corr(a,b,latency=latency)\n",
    "    # 2. Compute average empirical covariance between all pairs of trials\n",
    "    # -------------------------------------------------------------------------\n",
    "    S = np.zeros((n_chans, n_chans))\n",
    "    for i in range(n_chans):\n",
    "        for j in range(n_chans):\n",
    "            # n_samples, n_chans, n_trials \n",
    "            x_i = X[:, i, :]\n",
    "            x_j = X[:, j, :]\n",
    "            # print(f\"{x_i.shape=}\")\n",
    "            # print(f\"{x_j.shape=}\")\n",
    "            for t1 in range(n_trials):\n",
    "                for t2 in range(n_trials):\n",
    "                    if(t1 == t2): continue\n",
    "                    x_i_t1 = np.squeeze(x_i[:,t1])\n",
    "                    x_i_t1 -= x_i_t1.mean()\n",
    "\n",
    "                    x_j_t2 = np.squeeze(x_j[:,t2])\n",
    "                    x_j_t2 -= x_j_t2.mean()\n",
    "                    \n",
    "                    S[i,j] += get_corr(x_i_t1, x_j_t2, latency=latency)\n",
    "    # for trial_i in range(n_trials - 1):\n",
    "    #     x1 = np.squeeze(X[..., trial_i])\n",
    "\n",
    "    #     # Mean centering for the selected trial\n",
    "    #     x1 -= np.mean(x1, 0)\n",
    "\n",
    "    #     # Select a second trial that is different\n",
    "    #     for trial_j in range(trial_i + 1, n_trials):\n",
    "    #         x2 = np.squeeze(X[..., trial_j])\n",
    "\n",
    "    #         # Mean centering for the selected trial\n",
    "    #         x2 -= np.mean(x2, 0)\n",
    "\n",
    "    #         # Compute empirical covariance between the two selected trials and\n",
    "    #         # sum it\n",
    "    #         S = S + x1.T @ x2 + x2.T @ x1\n",
    "\n",
    "    # 3. Compute eigenvalues and vectors\n",
    "    # -------------------------------------------------------------------------\n",
    "    lambdas, W = linalg.eig(S, Q, left=True, right=False)\n",
    "\n",
    "    # Select the eigenvector corresponding to the biggest eigenvalue\n",
    "    W_best = W[:, np.argmax(lambdas)]\n",
    "\n",
    "    return W_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRCA:\n",
    "    \"\"\"Task-Related Component Analysis (TRCA).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sfreq : float\n",
    "        Sampling rate.\n",
    "    filterbank : list[[2-tuple, 2-tuple]]\n",
    "        Filterbank frequencies. Each list element is itself a list of passband\n",
    "        `Wp` and stopband `Ws` edges frequencies `[Wp, Ws]`. For example, this\n",
    "        creates 3 bands, starting at 6, 14, and 22 hz respectively::\n",
    "\n",
    "            [[(6, 90), (4, 100)],\n",
    "             [(14, 90), (10, 100)],\n",
    "             [(22, 90), (16, 100)]]\n",
    "\n",
    "        See :func:`scipy.signal.cheb1ord()` for more information on how to\n",
    "        specify the `Wp` and `Ws`.\n",
    "    ensemble : bool\n",
    "        If True, perform the ensemble TRCA analysis (default=False).\n",
    "    method : str in {'original'| 'riemann'}\n",
    "        Use original implementation from [1]_ or a variation that uses\n",
    "        regularization and the geodesic mean [2]_.\n",
    "    regularization : str in {'schaefer' | 'lwf' | 'oas' | 'scm'}\n",
    "        Regularization estimator used for covariance estimation with the\n",
    "        `riemann` method. Consider 'schaefer', 'lwf', 'oas'. 'scm' does not add\n",
    "        regularization and is almost equivalent to the original implementation.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    traindata : array, shape=(n_bands, n_chans, n_trials)\n",
    "        Reference (training) data decomposed into sub-band components by the\n",
    "        filter bank analysis.\n",
    "    y_train : array, shape=(n_trials)\n",
    "        Labels associated with the train data.\n",
    "    coef_ : array, shape=(n_chans, n_chans)\n",
    "        Weight coefficients for electrodes which can be used as a spatial\n",
    "        filter.\n",
    "    classes : list\n",
    "        Classes.\n",
    "    n_bands : int\n",
    "        Number of sub-bands.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] M. Nakanishi, Y. Wang, X. Chen, Y. -T. Wang, X. Gao, and T.-P. Jung,\n",
    "       \"Enhancing detection of SSVEPs for a high-speed brain speller using\n",
    "       task-related component analysis\", IEEE Trans. Biomed. Eng,\n",
    "       65(1):104-112, 2018.\n",
    "    .. [2] Barachant, A., Bonnet, S., Congedo, M., & Jutten, C. (2010,\n",
    "       October). Common spatial pattern revisited by Riemannian geometry. In\n",
    "       2010 IEEE International Workshop on Multimedia Signal Processing (pp.\n",
    "       472-476). IEEE.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sfreq, filterbank, ensemble=False, method='original',\n",
    "                 estimator='scm'):\n",
    "        self.sfreq = sfreq\n",
    "        self.ensemble = ensemble\n",
    "        self.filterbank = filterbank\n",
    "        self.n_bands = len(self.filterbank)\n",
    "        self.coef_ = None\n",
    "        self.method = method\n",
    "        if estimator == 'schaefer':\n",
    "            self.estimator = schaefer_strimmer_cov\n",
    "        else:\n",
    "            self.estimator = estimator\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Training stage of the TRCA-based SSVEP detection.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array, shape=(n_samples, n_chans[, n_trials])\n",
    "            Training EEG data.\n",
    "        y : array, shape=(trials,)\n",
    "            True label corresponding to each trial of the data array.\n",
    "\n",
    "        \"\"\"\n",
    "        n_samples, n_chans, _ = theshapeof(X)\n",
    "        classes = np.unique(y)\n",
    "\n",
    "        trains = np.zeros((len(classes), self.n_bands, n_samples, n_chans))\n",
    "\n",
    "        W = np.zeros((self.n_bands, len(classes), n_chans))\n",
    "\n",
    "        for class_i in classes:\n",
    "            # Select data with a specific label\n",
    "            eeg_tmp = X[..., y == class_i]\n",
    "            for fb_i in range(self.n_bands):\n",
    "                # Filter the signal with fb_i\n",
    "                eeg_tmp = bandpass(eeg_tmp, self.sfreq,\n",
    "                                   Wp=self.filterbank[fb_i][0],\n",
    "                                   Ws=self.filterbank[fb_i][1])\n",
    "                if (eeg_tmp.ndim == 3):\n",
    "                    # Compute mean of the signal across trials\n",
    "                    trains[class_i, fb_i] = np.mean(eeg_tmp, -1)\n",
    "                else:\n",
    "                    trains[class_i, fb_i] = eeg_tmp\n",
    "                # Find the spatial filter for the corresponding filtered signal\n",
    "                # and label\n",
    "                if self.method == 'original':\n",
    "                    w_best = _trca(eeg_tmp)\n",
    "                elif self.method == 'riemann':\n",
    "                    w_best = trca_regul(eeg_tmp, self.estimator)\n",
    "                elif self.method == 'crosscorrelation':\n",
    "                    w_best = trca_crosscorrelation(eeg_tmp)\n",
    "                else:\n",
    "                    raise ValueError('Invalid `method` option.')\n",
    "\n",
    "                W[fb_i, class_i, :] = w_best  # Store the spatial filter\n",
    "\n",
    "        self.trains = trains\n",
    "        self.coef_ = W\n",
    "        self.classes = classes\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Test phase of the TRCA-based SSVEP detection.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array, shape=(n_samples, n_chans[, n_trials])\n",
    "            Test data.\n",
    "        model: dict\n",
    "            Fitted model to be used in testing phase.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pred: np.array, shape (trials)\n",
    "            The target estimated by the method.\n",
    "\n",
    "        \"\"\"\n",
    "        if self.coef_ is None:\n",
    "            raise RuntimeError('TRCA is not fitted')\n",
    "\n",
    "        # Alpha coefficients for the fusion of filterbank analysis\n",
    "        fb_coefs = [(x + 1)**(-1.25) + 0.25 for x in range(self.n_bands)]\n",
    "        _, _, n_trials = theshapeof(X)\n",
    "\n",
    "        r = np.zeros((self.n_bands, len(self.classes)))\n",
    "        pred = np.zeros((n_trials), 'int')  # To store predictions\n",
    "\n",
    "        for trial in range(n_trials):\n",
    "            test_tmp = X[..., trial]  # pick a trial to be analysed\n",
    "            for fb_i in range(self.n_bands):\n",
    "\n",
    "                # filterbank on testdata\n",
    "                testdata = bandpass(test_tmp, self.sfreq,\n",
    "                                    Wp=self.filterbank[fb_i][0],\n",
    "                                    Ws=self.filterbank[fb_i][1])\n",
    "\n",
    "                for class_i in self.classes:\n",
    "                    # Retrieve reference signal for class i\n",
    "                    # (shape: n_chans, n_samples)\n",
    "                    traindata = np.squeeze(self.trains[class_i, fb_i])\n",
    "                    if self.ensemble:\n",
    "                        # shape = (n_chans, n_classes)\n",
    "                        w = np.squeeze(self.coef_[fb_i]).T\n",
    "                    else:\n",
    "                        # shape = (n_chans)\n",
    "                        w = np.squeeze(self.coef_[fb_i, class_i])\n",
    "\n",
    "                    # Compute 2D correlation of spatially filtered test data\n",
    "                    # with ref\n",
    "                    r_tmp = np.corrcoef((testdata @ w).flatten(),\n",
    "                                        (traindata @ w).flatten())\n",
    "                    r[fb_i, class_i] = r_tmp[0, 1]\n",
    "\n",
    "            rho = np.dot(fb_coefs, r)  # fusion for the filterbank analysis\n",
    "\n",
    "            tau = np.argmax(rho)  # retrieving index of the max\n",
    "            pred[trial] = int(tau)\n",
    "\n",
    "        return pred\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 1505)\n",
      "(9, 1501)\n",
      "(9, 1502)\n",
      "(9, 1505)\n",
      "(9, 1508)\n",
      "(9, 1502)\n",
      "(9, 1507)\n",
      "(9, 1502)\n",
      "(9, 1500)\n",
      "(9, 1506)\n",
      "(9, 1510)\n",
      "(9, 1503)\n",
      "(9, 1502)\n",
      "(9, 1504)\n",
      "(9, 1508)\n",
      "(9, 1502)\n",
      "(9, 1505)\n",
      "(9, 1500)\n",
      "(9, 1503)\n",
      "(9, 1508)\n",
      "(9, 1502)\n",
      "(9, 1510)\n",
      "(9, 1507)\n",
      "(9, 1506)\n",
      "(24, 8, 1240) (24,)\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "from scipy import signal\n",
    "\n",
    "BOARD_ID = 8\n",
    "loaded_model = pickle.load(open(r\"C:\\Users\\bci\\Documents\\projects\\hybrid-ssvep-p300-speller\\hybrid\\nakanishi_TRCA_model.sav\", 'rb'))\n",
    "# pickle_files = glob(r'E:\\Thesis\\HybridSpeller\\nine_flicker\\record\\farheen_20230427_v2\\*.pickle')\n",
    "pickle_files = glob(r'C:\\Users\\bci\\Documents\\projects\\hybrid-ssvep-p300-speller\\hybrid\\record\\final\\farheen_20230616_2sec_0.5_overlap_8Hz_8target\\*.pickle')\n",
    "\n",
    "y_pickle = []\n",
    "X_pickle = []\n",
    "for fpath in pickle_files:\n",
    "    _,filename = os.path.split(fpath)\n",
    "    y_pickle.append(filename[1])\n",
    "\n",
    "    with open(fpath, 'rb') as handle:\n",
    "        pickle_data = pickle.load(handle)\n",
    "\n",
    "    marker_channel = BoardShim.get_marker_channel(BOARD_ID)\n",
    "    eeg_channels = BoardShim.get_eeg_channels(BOARD_ID)\n",
    "    pickle_data[eeg_channels] = pickle_data[eeg_channels] / 1e6\n",
    "    pickle_data = pickle_data[eeg_channels + [marker_channel]]\n",
    "    print(pickle_data.shape)\n",
    "\n",
    "    _CHANNELS = ['FZ', 'C3', 'CZ', 'C4', 'PZ', 'PO7', 'OZ', 'PO8']\n",
    "\n",
    "    data = pickle_data[:8,:1240]\n",
    "\n",
    "\n",
    "    b,a = signal.iirfilter(7, Wn=[1, 92], rp=0.5, btype='band', analog=False, fs=250,  ftype='cheby1')\n",
    "    data = signal.filtfilt(b,a,data,axis=1)\n",
    "\n",
    "    notch_freq = 50\n",
    "    quality = 1\n",
    "    b,a = signal.iirnotch(notch_freq, quality, fs=250)\n",
    "    for i in range(8):\n",
    "        data[i] = signal.lfilter(b, a, data[i])\n",
    "\n",
    "    X_pickle.append(np.expand_dims(data[:],axis=0))\n",
    "\n",
    "y_pickle = np.array(y_pickle)\n",
    "X_pickle = np.concatenate(X_pickle)\n",
    "\n",
    "print(X_pickle.shape, y_pickle.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 8, 1240)\n"
     ]
    }
   ],
   "source": [
    "print(X_pickle.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'A' 'B'\n",
      " 'C' 'D' 'E' 'F' 'G' 'H']\n"
     ]
    }
   ],
   "source": [
    "print(y_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_predict = None\n",
    "best_score = 0\n",
    "X_pickle = np.swapaxes(X_pickle,0,2)\n",
    "print(X_pickle.shape)\n",
    "for r in range(500):\n",
    "    offset = int(r)\n",
    "    # offset = int(r)\n",
    "    preds_pickle = loaded_model.predict(X_pickle[offset:offset + 1000,:,:])\n",
    "    # print(preds_pickle)\n",
    "    # print(y_pickle)\n",
    "    # print(X_pickle[:,:,offset:offset + 750].shape)\n",
    "    c = 0\n",
    "    preds_pickle_converted = []\n",
    "    for idx in range(len(preds_pickle)):\n",
    "        pred = preds_pickle[idx]\n",
    "        label = y_pickle[idx]\n",
    "        if(pred == 0):\n",
    "            pred = 'A'\n",
    "        if(pred == 1):\n",
    "            pred = 'B'\n",
    "        if(pred == 2):\n",
    "            pred = 'C'\n",
    "        if(pred == 3):\n",
    "            pred = 'D'\n",
    "        if(pred == 4):\n",
    "            pred = 'E'\n",
    "        if(pred == 5):\n",
    "            pred = 'F'\n",
    "        if(pred == 6):\n",
    "            pred = 'G'\n",
    "        if(pred == 7):\n",
    "            pred = 'H'\n",
    "        if(pred == 8):\n",
    "            pred = 'I'\n",
    "        if(pred == 9):\n",
    "            pred = 'J'\n",
    "        if(pred == 10):\n",
    "            pred = 'K'\n",
    "        if(pred == 11):\n",
    "            pred = 'L'\n",
    "        if(pred == 12):\n",
    "            pred = 'M'\n",
    "        if(pred == 13):\n",
    "            pred = 'N'\n",
    "        if(pred == 14):\n",
    "            pred = 'O'\n",
    "        if(pred == 15):\n",
    "            pred = 'P'\n",
    "        if(label == pred):\n",
    "            c += 1\n",
    "        # print(idx, f\"{pred=} {label=}\" )\n",
    "    score = c/len(preds_pickle)\n",
    "    print(offset, score)\n",
    "    if(best_score < score):\n",
    "        best_score = score\n",
    "        best_predict = preds_pickle_converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1240, 8, 24)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 8000 and the array at index 1 has size 6000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m X_pickle \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mswapaxes(X_pickle,\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(X_pickle\u001b[39m.\u001b[39mshape)\n\u001b[1;32m----> 4\u001b[0m preds_pickle \u001b[39m=\u001b[39m loaded_model\u001b[39m.\u001b[39;49mpredict(X_pickle[offset:offset \u001b[39m+\u001b[39;49m \u001b[39m1000\u001b[39;49m,:,:])\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(preds_pickle)\n\u001b[0;32m      6\u001b[0m \u001b[39m# print(y_pickle)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[54], line 168\u001b[0m, in \u001b[0;36mTRCA.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    164\u001b[0m             w \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqueeze(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_[fb_i, class_i])\n\u001b[0;32m    166\u001b[0m         \u001b[39m# Compute 2D correlation of spatially filtered test data\u001b[39;00m\n\u001b[0;32m    167\u001b[0m         \u001b[39m# with ref\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m         r_tmp \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mcorrcoef((testdata \u001b[39m@\u001b[39;49m w)\u001b[39m.\u001b[39;49mflatten(),\n\u001b[0;32m    169\u001b[0m                             (traindata \u001b[39m@\u001b[39;49m w)\u001b[39m.\u001b[39;49mflatten())\n\u001b[0;32m    170\u001b[0m         r[fb_i, class_i] \u001b[39m=\u001b[39m r_tmp[\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m]\n\u001b[0;32m    172\u001b[0m rho \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(fb_coefs, r)  \u001b[39m# fusion for the filterbank analysis\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mcorrcoef\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\bci\\.virtualenvs\\hybrid-ssvep-p300-speller-ZL_XZSnA\\lib\\site-packages\\numpy\\lib\\function_base.py:2845\u001b[0m, in \u001b[0;36mcorrcoef\u001b[1;34m(x, y, rowvar, bias, ddof, dtype)\u001b[0m\n\u001b[0;32m   2841\u001b[0m \u001b[39mif\u001b[39;00m bias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39m_NoValue \u001b[39mor\u001b[39;00m ddof \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39m_NoValue:\n\u001b[0;32m   2842\u001b[0m     \u001b[39m# 2015-03-15, 1.10\u001b[39;00m\n\u001b[0;32m   2843\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m'\u001b[39m\u001b[39mbias and ddof have no effect and are deprecated\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   2844\u001b[0m                   \u001b[39mDeprecationWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m-> 2845\u001b[0m c \u001b[39m=\u001b[39m cov(x, y, rowvar, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m   2846\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   2847\u001b[0m     d \u001b[39m=\u001b[39m diag(c)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mcov\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\bci\\.virtualenvs\\hybrid-ssvep-p300-speller-ZL_XZSnA\\lib\\site-packages\\numpy\\lib\\function_base.py:2639\u001b[0m, in \u001b[0;36mcov\u001b[1;34m(m, y, rowvar, bias, ddof, fweights, aweights, dtype)\u001b[0m\n\u001b[0;32m   2637\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m rowvar \u001b[39mand\u001b[39;00m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   2638\u001b[0m         y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mT\n\u001b[1;32m-> 2639\u001b[0m     X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mconcatenate((X, y), axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m   2641\u001b[0m \u001b[39mif\u001b[39;00m ddof \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2642\u001b[0m     \u001b[39mif\u001b[39;00m bias \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 8000 and the array at index 1 has size 6000"
     ]
    }
   ],
   "source": [
    "offset = 225\n",
    "X_pickle = np.swapaxes(X_pickle,0,2)\n",
    "print(X_pickle.shape)\n",
    "preds_pickle = loaded_model.predict(X_pickle[offset:offset + 1000,:,:])\n",
    "print(preds_pickle)\n",
    "# print(y_pickle)\n",
    "c = 0\n",
    "for idx in range(len(preds_pickle)):\n",
    "    pred = preds_pickle[idx]\n",
    "    label = y_pickle[idx]\n",
    "    if(pred == 0):\n",
    "        pred = 'A'\n",
    "    if(pred == 1):\n",
    "        pred = 'B'\n",
    "    if(pred == 2):\n",
    "        pred = 'C'\n",
    "    if(pred == 3):\n",
    "        pred = 'D'\n",
    "    if(pred == 4):\n",
    "        pred = 'E'\n",
    "    if(pred == 5):\n",
    "        pred = 'F'\n",
    "    if(pred == 6):\n",
    "        pred = 'G'\n",
    "    if(pred == 7):\n",
    "        pred = 'H'\n",
    "    if(pred == 8):\n",
    "        pred = 'I'\n",
    "    if(pred == 9):\n",
    "        pred = 'J'\n",
    "    if(pred == 10):\n",
    "        pred = 'K'\n",
    "    if(pred == 11):\n",
    "        pred = 'L'\n",
    "    if(pred == 12):\n",
    "        pred = 'M'\n",
    "    if(pred == 13):\n",
    "        pred = 'N'\n",
    "    if(pred == 14):\n",
    "        pred = 'O'\n",
    "    if(pred == 15):\n",
    "        pred = 'P'\n",
    "    if(label == pred):\n",
    "        c += 1\n",
    "    pred_ascii = ord(pred)-65\n",
    "    label_ascii = ord(label)-65\n",
    "    print(idx, f\"{pred=} {pred_ascii} {label=} {label_ascii}\" )\n",
    "print(c/len(preds_pickle)\n",
    "      )\n",
    "print(len(preds_pickle))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HybridSpeller-q8UBACmb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
